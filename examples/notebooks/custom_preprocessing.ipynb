{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlantic - Detailed Preprocessing Components\n",
    "\n",
    "This notebook demonstrates individual usage of all preprocessing components:\n",
    "\n",
    "1. **Date Engineering** - Extract temporal features from datetime columns\n",
    "2. **Encoders** - Label, IFrequency, OneHot\n",
    "3. **Scalers** - Standard, MinMax, Robust\n",
    "4. **Imputers** - Simple, KNN, Iterative\n",
    "5. **Feature Selectors** - H2O, VIF\n",
    "6. **Registries** - Component factories\n",
    "7. **Encoding Versions** - Pre-built encoding strategies\n",
    "8. **Metrics** - Evaluation utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from atlantic.data import DatasetGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset with mixed types\n",
    "data, target = DatasetGenerator.generate_with_datetime(\n",
    "    n_samples=1000,\n",
    "    n_numeric=8,\n",
    "    n_categorical=4,\n",
    "    task_type=\"classification\",\n",
    "    null_percentage=0.10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train, test = train_test_split(data, train_size=0.8, random_state=42)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset Shape: {data.shape}\")\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"\\nColumn Types:\")\n",
    "print(f\"  Numeric: {train.select_dtypes(include=['int', 'float']).columns.tolist()[:5]}...\")\n",
    "print(f\"  Categorical: {train.select_dtypes(include=['object', 'category']).columns.tolist()}\")\n",
    "print(f\"  Datetime: {train.select_dtypes(include=['datetime64']).columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Date Engineering\n",
    "\n",
    "Extract temporal features from datetime columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlantic.utils.datetime import engineer_datetime_features, DATE_COMPONENTS\n",
    "\n",
    "print(f\"Available Date Components: {DATE_COMPONENTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply date engineering with specific components\n",
    "train_dates = engineer_datetime_features(\n",
    "    train.copy(),\n",
    "    drop_original=True,\n",
    "    components=['day_of_week', 'month', 'year', 'is_wknd']\n",
    ")\n",
    "test_dates = engineer_datetime_features(\n",
    "    test.copy(),\n",
    "    drop_original=True,\n",
    "    components=['day_of_week', 'month', 'year', 'is_wknd']\n",
    ")\n",
    "\n",
    "print(f\"Before Date Engineering: {train.shape}\")\n",
    "print(f\"After Date Engineering: {train_dates.shape}\")\n",
    "\n",
    "# Show generated columns\n",
    "date_generated = [col for col in train_dates.columns if any(\n",
    "    comp in col for comp in ['day_of_week', 'month', 'year', 'is_wknd']\n",
    ")]\n",
    "print(f\"Generated Date Features: {date_generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full date engineering (all components)\n",
    "train_full_dates = engineer_datetime_features(train.copy(), drop_original=True)\n",
    "test_full_dates = engineer_datetime_features(test.copy(), drop_original=True)\n",
    "print(f\"With All Components - Train: {train_full_dates.shape}, Test: {test_full_dates.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Encoders\n",
    "\n",
    "Convert categorical variables to numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlantic.preprocessing import (\n",
    "    AutoLabelEncoder,\n",
    "    AutoIFrequencyEncoder,\n",
    "    AutoOneHotEncoder\n",
    ")\n",
    "from atlantic.utils.columns import get_categorical_columns\n",
    "\n",
    "# Get categorical columns (excluding target)\n",
    "cat_cols = get_categorical_columns(train_full_dates, exclude=[target])\n",
    "print(f\"Categorical Columns: {cat_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Label Encoder\n",
    "\n",
    "Maps categories to ordinal integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = AutoLabelEncoder()\n",
    "label_encoder.fit(train_full_dates[cat_cols])\n",
    "\n",
    "train_label = train_full_dates.copy()\n",
    "test_label = test_full_dates.copy()\n",
    "train_label[cat_cols] = label_encoder.transform(train_full_dates[cat_cols])\n",
    "test_label[cat_cols] = label_encoder.transform(test_full_dates[cat_cols])\n",
    "\n",
    "print(f\"Original Categories (first col): {train_full_dates[cat_cols[0]].unique()[:5]}\")\n",
    "print(f\"Encoded Values (first col): {train_label[cat_cols[0]].unique()[:5]}\")\n",
    "\n",
    "# Inverse transform\n",
    "train_inverse = label_encoder.inverse_transform(train_label[cat_cols])\n",
    "print(f\"Inverse Transform: {train_inverse[cat_cols[0]].unique()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 IFrequency Encoder\n",
    "\n",
    "Encodes categories based on inverse document frequency weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifreq_encoder = AutoIFrequencyEncoder()\n",
    "ifreq_encoder.fit(train_full_dates[cat_cols])\n",
    "\n",
    "train_ifreq = train_full_dates.copy()\n",
    "test_ifreq = test_full_dates.copy()\n",
    "train_ifreq[cat_cols] = ifreq_encoder.transform(train_full_dates[cat_cols])\n",
    "test_ifreq[cat_cols] = ifreq_encoder.transform(test_full_dates[cat_cols])\n",
    "\n",
    "print(f\"IFrequency Encoded (first col sample):\")\n",
    "train_ifreq[cat_cols[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 OneHot Encoder\n",
    "\n",
    "Creates binary columns for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = AutoOneHotEncoder()\n",
    "onehot_encoder.fit(train_full_dates[cat_cols])\n",
    "\n",
    "train_onehot = onehot_encoder.transform(train_full_dates[cat_cols])\n",
    "test_onehot = onehot_encoder.transform(test_full_dates[cat_cols])\n",
    "\n",
    "print(f\"Before OneHot: {train_full_dates.shape}\")\n",
    "print(f\"After OneHot - Train: {train_onehot.shape}, Test: {test_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Scalers\n",
    "\n",
    "Normalize numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlantic.preprocessing import (\n",
    "    AutoMinMaxScaler,\n",
    "    AutoStandardScaler,\n",
    "    AutoRobustScaler\n",
    ")\n",
    "from atlantic.utils.columns import get_numeric_columns\n",
    "\n",
    "# Get numeric columns\n",
    "num_cols = get_numeric_columns(train_label, exclude=[target])\n",
    "print(f\"Numeric Columns: {num_cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Standard Scaler\n",
    "\n",
    "Zero mean, unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = AutoStandardScaler()\n",
    "std_scaler.fit(train_label[num_cols])\n",
    "\n",
    "train_std = std_scaler.transform(train_label[num_cols])\n",
    "test_std = std_scaler.transform(test_label[num_cols])\n",
    "\n",
    "print(f\"Original Mean: {train_label[num_cols[0]].mean():.4f}\")\n",
    "print(f\"Scaled Mean: {train_std[num_cols[0]].mean():.4f}\")\n",
    "print(f\"Original Std: {train_label[num_cols[0]].std():.4f}\")\n",
    "print(f\"Scaled Std: {train_std[num_cols[0]].std():.4f}\")\n",
    "\n",
    "# Inverse transform\n",
    "train_std_inverse = std_scaler.inverse_transform(train_std)\n",
    "print(f\"Inverse Mean: {train_std_inverse[num_cols[0]].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MinMax Scaler\n",
    "\n",
    "Scales to [0, 1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = AutoMinMaxScaler()\n",
    "minmax_scaler.fit(train_label[num_cols])\n",
    "\n",
    "train_minmax = minmax_scaler.transform(train_label[num_cols])\n",
    "test_minmax = minmax_scaler.transform(test_label[num_cols])\n",
    "\n",
    "print(f\"Original Range: [{train_label[num_cols[0]].min():.2f}, {train_label[num_cols[0]].max():.2f}]\")\n",
    "print(f\"Scaled Range: [{train_minmax[num_cols[0]].min():.2f}, {train_minmax[num_cols[0]].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Robust Scaler\n",
    "\n",
    "Median and IQR based - resistant to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler = AutoRobustScaler()\n",
    "robust_scaler.fit(train_label[num_cols])\n",
    "\n",
    "train_robust = robust_scaler.transform(train_label[num_cols])\n",
    "test_robust = robust_scaler.transform(test_label[num_cols])\n",
    "\n",
    "print(f\"Original Median: {train_label[num_cols[0]].median():.4f}\")\n",
    "print(f\"Scaled Median: {train_robust[num_cols[0]].median():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Imputers\n",
    "\n",
    "Handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlantic.preprocessing import (\n",
    "    AutoSimpleImputer,\n",
    "    AutoKNNImputer,\n",
    "    AutoIterativeImputer\n",
    ")\n",
    "\n",
    "# Check missing values\n",
    "print(\"Missing Values Before Imputation:\")\n",
    "print(f\"  Train Total: {train_label.select_dtypes(include=['float', 'int']).isna().sum().sum()}\")\n",
    "print(f\"  Test Total: {test_label.select_dtypes(include=['float', 'int']).isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Simple Imputer\n",
    "\n",
    "Statistical imputation (mean, median, mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer = AutoSimpleImputer(strategy='mean', target=target)\n",
    "simple_imputer.fit(train_label)\n",
    "\n",
    "train_simple = simple_imputer.transform(train_label.copy())\n",
    "test_simple = simple_imputer.transform(test_label.copy())\n",
    "\n",
    "print(f\"Missing After (Train): {train_simple.select_dtypes(include=['float', 'int']).isna().sum().sum()}\")\n",
    "print(f\"Missing After (Test): {test_simple.select_dtypes(include=['float', 'int']).isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 KNN Imputer\n",
    "\n",
    "Imputes using K-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer = AutoKNNImputer(n_neighbors=3, weights=\"uniform\", target=target)\n",
    "knn_imputer.fit(train_label)\n",
    "\n",
    "train_knn = knn_imputer.transform(train_label.copy())\n",
    "test_knn = knn_imputer.transform(test_label.copy())\n",
    "\n",
    "print(f\"Missing After (Train): {train_knn.select_dtypes(include=['float', 'int']).isna().sum().sum()}\")\n",
    "print(f\"Missing After (Test): {test_knn.select_dtypes(include=['float', 'int']).isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Iterative Imputer\n",
    "\n",
    "Multivariate imputation using iterative modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_imputer = AutoIterativeImputer(\n",
    "    max_iter=10,\n",
    "    random_state=42,\n",
    "    initial_strategy=\"mean\",\n",
    "    imputation_order=\"ascending\",\n",
    "    target=target\n",
    ")\n",
    "iter_imputer.fit(train_label)\n",
    "\n",
    "train_iter = iter_imputer.transform(train_label.copy())\n",
    "test_iter = iter_imputer.transform(test_label.copy())\n",
    "\n",
    "print(f\"Missing After (Train): {train_iter.select_dtypes(include=['float', 'int']).isna().sum().sum()}\")\n",
    "print(f\"Missing After (Test): {test_iter.select_dtypes(include=['float', 'int']).isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Selection\n",
    "\n",
    "Remove redundant or multicollinear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlantic.feature_selection import VIFFeatureSelector\n",
    "\n",
    "# Prepare data for VIF (needs numeric, no nulls)\n",
    "train_for_vif = train_simple.copy()\n",
    "test_for_vif = test_simple.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 VIF Feature Selection\n",
    "\n",
    "Removes multicollinear features based on Variance Inflation Factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_selector = VIFFeatureSelector(target=target, vif_threshold=10.0)\n",
    "\n",
    "try:\n",
    "    vif_selector.fit(train_for_vif)\n",
    "    print(f\"Original Features: {len(num_cols)}\")\n",
    "    print(f\"Selected Features: {vif_selector.n_selected}\")\n",
    "    print(f\"Features Removed: {vif_selector.n_removed}\")\n",
    "    print(f\"Selected: {vif_selector.selected_features[:5]}...\")\n",
    "    \n",
    "    # Transform both train and test\n",
    "    train_vif = vif_selector.transform(train_for_vif)\n",
    "    test_vif = vif_selector.transform(test_for_vif)\n",
    "    print(f\"After VIF - Train: {train_vif.shape}, Test: {test_vif.shape}\")\n",
    "    \n",
    "    if vif_selector.vif_dataframe is not None:\n",
    "        print(\"\\nVIF Values (top 5):\")\n",
    "        display(vif_selector.vif_dataframe.head())\n",
    "except Exception as e:\n",
    "    print(f\"VIF Selection Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Component Registries\n",
    "\n",
    "Factory pattern for creating components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlantic.preprocessing.registry import (\n",
    "    EncoderRegistry,\n",
    "    ScalerRegistry,\n",
    "    ImputerRegistry\n",
    ")\n",
    "\n",
    "print(\"Available Components:\")\n",
    "print(f\"  Encoders: {EncoderRegistry.list_available()}\")\n",
    "print(f\"  Scalers: {ScalerRegistry.list_available()}\")\n",
    "print(f\"  Imputers: {ImputerRegistry.list_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get components from registry\n",
    "encoder = EncoderRegistry.get(\"ifrequency\")\n",
    "scaler = ScalerRegistry.get(\"standard\")\n",
    "imputer = ImputerRegistry.get(\"knn\", n_neighbors=5, target=target)\n",
    "\n",
    "print(\"Created from Registry:\")\n",
    "print(f\"  Encoder: {type(encoder).__name__}\")\n",
    "print(f\"  Scaler: {type(scaler).__name__}\")\n",
    "print(f\"  Imputer: {type(imputer).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Encoding Versions\n",
    "\n",
    "Pre-built encoding strategies combining scalers and encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlantic.encoding import EncodingVersionFactory, EncodingVersion\n",
    "\n",
    "print(\"Available Encoding Versions:\")\n",
    "for version in EncodingVersionFactory.list_versions():\n",
    "    desc = EncodingVersionFactory.describe_version(version)\n",
    "    print(f\"  {version.upper()}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply specific version\n",
    "ev = EncodingVersion(train=train_simple.copy(), test=test_simple.copy(), target=target)\n",
    "\n",
    "print(\"Applying Encoding Version 1 (Standard + IFrequency):\")\n",
    "train_v1, test_v1 = ev.encoding_v1()\n",
    "print(f\"  Train Shape: {train_v1.shape}, Test Shape: {test_v1.shape}\")\n",
    "\n",
    "print(\"\\nApplying Encoding Version 4 (MinMax + Label):\")\n",
    "train_v4, test_v4 = ev.encoding_v4()\n",
    "print(f\"  Train Shape: {train_v4.shape}, Test Shape: {test_v4.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlantic.evaluation import MetricRegistry, metrics_classification\n",
    "from atlantic.core.enums import TaskType\n",
    "\n",
    "print(\"Available Metrics:\")\n",
    "print(f\"  Regression: {MetricRegistry.list_available(TaskType.REGRESSION)}\")\n",
    "print(f\"  Classification: {MetricRegistry.list_available(TaskType.CLASSIFICATION)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example metric usage\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1])\n",
    "y_pred = np.array([1, 0, 0, 1, 0, 1])\n",
    "\n",
    "cls_metrics = metrics_classification(y_true, y_pred, n_classes=2)\n",
    "print(\"Classification Metrics Example:\")\n",
    "cls_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Components Available\n",
    "\n",
    "| Category | Components |\n",
    "|----------|------------|\n",
    "| **Encoders** | AutoLabelEncoder, AutoIFrequencyEncoder, AutoOneHotEncoder |\n",
    "| **Scalers** | AutoStandardScaler, AutoMinMaxScaler, AutoRobustScaler |\n",
    "| **Imputers** | AutoSimpleImputer, AutoKNNImputer, AutoIterativeImputer |\n",
    "| **Feature Selectors** | H2OFeatureSelector, VIFFeatureSelector |\n",
    "\n",
    "### Utilities\n",
    "\n",
    "| Utility | Function |\n",
    "|---------|----------|\n",
    "| Date Engineering | `engineer_datetime_features()` |\n",
    "| Column Detection | `get_numeric_columns()`, `get_categorical_columns()` |\n",
    "| Registries | EncoderRegistry, ScalerRegistry, ImputerRegistry |\n",
    "| Encoding Versions | EncodingVersionFactory (V1-V4) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
